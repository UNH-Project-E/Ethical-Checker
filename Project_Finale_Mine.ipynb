{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project Finale Mine.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Id79WBN6XIH3"
      },
      "source": [
        "import requests\r\n",
        "import RAKE\r\n",
        "import operator\r\n",
        "API_KEY = \"AIzaSyAvZlAZHcE3lXigI2ZJxWQvCoCpF6qFiyY\"\r\n",
        "SEARCH_ENGINE_ID = \"bf6585a79f0c6d8bb\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T6ymLlKvnTiW",
        "outputId": "d21fcfa6-9039-4138-dbb5-b5cad2e3545b"
      },
      "source": [
        "pip install python-rake==1.4.4"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting python-rake==1.4.4\n",
            "  Downloading https://files.pythonhosted.org/packages/36/b2/688c902c289f32207b2dc56e4b4c713a4a04d68e23ec8216e936fea1aded/python-rake-1.4.4.tar.gz\n",
            "Building wheels for collected packages: python-rake\n",
            "  Building wheel for python-rake (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-rake: filename=python_rake-1.4.4-cp37-none-any.whl size=13460 sha256=ff098453afbb9bf1ad0853b52dd212f35f895535ccfd4049a81f852ad9035cbe\n",
            "  Stored in directory: /root/.cache/pip/wheels/60/50/ec/b2aa3280e913e091644c7d436fc68019214c7e72998b7994db\n",
            "Successfully built python-rake\n",
            "Installing collected packages: python-rake\n",
            "Successfully installed python-rake-1.4.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HJuJezPjTW--",
        "outputId": "90b95990-838d-4ca2-f5d6-1526614484ba"
      },
      "source": [
        "pip install rake-nltk"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting rake-nltk\n",
            "  Downloading https://files.pythonhosted.org/packages/8e/c4/b4ff57e541ac5624ad4b20b89c2bafd4e98f29fd83139f3a81858bdb3815/rake_nltk-1.0.4.tar.gz\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from rake-nltk) (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->rake-nltk) (1.15.0)\n",
            "Building wheels for collected packages: rake-nltk\n",
            "  Building wheel for rake-nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rake-nltk: filename=rake_nltk-1.0.4-py2.py3-none-any.whl size=7819 sha256=f8a027fbadb99d7be352d2e9e12d7758fbf8f6db19e64b489b68af3299744bfb\n",
            "  Stored in directory: /root/.cache/pip/wheels/ef/92/fc/271b3709e71a96ffe934b27818946b795ac6b9b8ff8682483f\n",
            "Successfully built rake-nltk\n",
            "Installing collected packages: rake-nltk\n",
            "Successfully installed rake-nltk-1.0.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sTS8QYWxaDOQ",
        "outputId": "a96a03e1-543c-4fbb-92b9-daab5bfc43aa"
      },
      "source": [
        "pip install sentence-transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sentence-transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6a/e2/84d6acfcee2d83164149778a33b6bdd1a74e1bcb59b2b2cd1b861359b339/sentence-transformers-0.4.1.2.tar.gz (64kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 3.6MB/s \n",
            "\u001b[?25hCollecting transformers<5.0.0,>=3.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f9/54/5ca07ec9569d2f232f3166de5457b63943882f7950ddfcc887732fc7fb23/transformers-4.3.3-py3-none-any.whl (1.9MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9MB 6.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.41.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.7.1+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.19.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.4.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (3.2.5)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 20.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (20.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (3.0.12)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 40.4MB/s \n",
            "\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/23/2ddc317b2121117bf34dd00f5b0de194158f2a44ee2bf5e47c7166878a97/tokenizers-0.10.1-cp37-cp37m-manylinux2010_x86_64.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 45.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (3.7.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->sentence-transformers) (3.7.4.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sentence-transformers) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->sentence-transformers) (1.15.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers<5.0.0,>=3.1.0->sentence-transformers) (2.4.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=3.1.0->sentence-transformers) (7.1.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers<5.0.0,>=3.1.0->sentence-transformers) (3.4.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers) (2020.12.5)\n",
            "Building wheels for collected packages: sentence-transformers, sacremoses\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-0.4.1.2-cp37-none-any.whl size=103068 sha256=7e6369bb4a6ff6d156bfbba23f2bef7587de3e85e0862fe3af93cac5d957b671\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/33/d1/5703dd56199c09d4a1b41e0c07fb4e7765a84d787cbdc48ac3\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp37-none-any.whl size=893262 sha256=a2a0a7b4beee8d7146f0843d5d15870d03439cd466a1e5a67da134f53f38d116\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sentence-transformers sacremoses\n",
            "Installing collected packages: sacremoses, tokenizers, transformers, sentencepiece, sentence-transformers\n",
            "Successfully installed sacremoses-0.0.43 sentence-transformers-0.4.1.2 sentencepiece-0.1.95 tokenizers-0.10.1 transformers-4.3.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 925
        },
        "id": "u-PCANMLiDxq",
        "outputId": "fa9e77b7-a317-4df6-b435-e40aea24a751"
      },
      "source": [
        "pip install trafilatura\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting trafilatura\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/02/a6/5cb251a3799e6ea7aae67fd2a7b7a7fff2bbb9360ba98c9887418ba47945/trafilatura-0.8.0-py3-none-any.whl (161kB)\n",
            "\r\u001b[K     |██                              | 10kB 14.1MB/s eta 0:00:01\r\u001b[K     |████                            | 20kB 17.1MB/s eta 0:00:01\r\u001b[K     |██████                          | 30kB 10.4MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 40kB 8.5MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 51kB 4.3MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 61kB 5.0MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 71kB 5.2MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 81kB 5.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 92kB 5.3MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 102kB 4.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 112kB 4.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 122kB 4.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 133kB 4.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 143kB 4.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 153kB 4.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 163kB 4.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet>=3.0.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from trafilatura) (3.0.4)\n",
            "Collecting courlan>=0.3.1\n",
            "  Downloading https://files.pythonhosted.org/packages/f8/61/bb440c980fcacbf7d1bf09423c54c6f25b6559ae492ba8cff2081eff3e04/courlan-0.3.1-py3-none-any.whl\n",
            "Collecting htmldate>=0.8.0\n",
            "  Downloading https://files.pythonhosted.org/packages/ff/31/8c281b6dabc61f2f4ec8b138580148ace1df15b40485ee8b7fe8789d4d12/htmldate-0.8.0-py3-none-any.whl\n",
            "Collecting readability-lxml>=0.8.1\n",
            "  Downloading https://files.pythonhosted.org/packages/39/a6/cfe22aaa19ac69b97d127043a76a5bbcb0ef24f3a0b22793c46608190caa/readability_lxml-0.8.1-py3-none-any.whl\n",
            "Collecting lxml>=4.6.2; python_version > \"3.4\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d2/88/b25778f17e5320c1c58f8c5060fb5b037288e162bd7554c30799e9ea90db/lxml-4.6.2-cp37-cp37m-manylinux1_x86_64.whl (5.5MB)\n",
            "\u001b[K     |████████████████████████████████| 5.5MB 27.7MB/s \n",
            "\u001b[?25hCollecting urllib3<2,>=1.25\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/23/fc/8a49991f7905261f9ca9df5aa9b58363c3c821ce3e7f671895442b7100f2/urllib3-1.26.3-py2.py3-none-any.whl (137kB)\n",
            "\u001b[K     |████████████████████████████████| 143kB 23.8MB/s \n",
            "\u001b[?25hCollecting justext>=2.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6c/5f/c7b909b4b864ebcacfac23ce2f6f01a50c53628787cc14b3c06f79464cab/jusText-2.2.0-py2.py3-none-any.whl (860kB)\n",
            "\u001b[K     |████████████████████████████████| 870kB 35.4MB/s \n",
            "\u001b[?25hCollecting tld; python_version >= \"3.6\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1f/51/ec8741d354a59450327be40591ef50b0ddb78bfb359fe1319003b233e5c8/tld-0.12.5-py37-none-any.whl (408kB)\n",
            "\u001b[K     |████████████████████████████████| 409kB 33.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.7/dist-packages (from htmldate>=0.8.0->trafilatura) (2.8.1)\n",
            "Collecting dateparser>=1.0.0; python_version > \"3.4\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/78/c4/b5ddc3eeac974d85055d88c1e6b62cc492fc1a93dbe3b66a45a756a7b807/dateparser-1.0.0-py2.py3-none-any.whl (279kB)\n",
            "\u001b[K     |████████████████████████████████| 286kB 41.9MB/s \n",
            "\u001b[?25hCollecting regex>=2020.11.13\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/61/b2/8f281520d9f08d0f6771b8160a87a4b487850cde9f1abe257da4d8bab599/regex-2020.11.13-cp37-cp37m-manylinux2014_x86_64.whl (719kB)\n",
            "\u001b[K     |████████████████████████████████| 727kB 52.9MB/s \n",
            "\u001b[?25hCollecting cssselect\n",
            "  Downloading https://files.pythonhosted.org/packages/3b/d4/3b5c17f00cce85b9a1e6f91096e1cc8e8ede2e1be8e96b87ce1ed09e92c5/cssselect-1.1.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.8.1->htmldate>=0.8.0->trafilatura) (1.15.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from dateparser>=1.0.0; python_version > \"3.4\"->htmldate>=0.8.0->trafilatura) (2018.9)\n",
            "Requirement already satisfied: tzlocal in /usr/local/lib/python3.7/dist-packages (from dateparser>=1.0.0; python_version > \"3.4\"->htmldate>=0.8.0->trafilatura) (1.5.1)\n",
            "\u001b[31mERROR: requests 2.23.0 has requirement urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1, but you'll have urllib3 1.26.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: htmldate 0.8.0 has requirement chardet>=4.0.0; python_version >= \"3.6\", but you'll have chardet 3.0.4 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tld, urllib3, courlan, lxml, regex, dateparser, htmldate, cssselect, readability-lxml, justext, trafilatura\n",
            "  Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Found existing installation: lxml 4.2.6\n",
            "    Uninstalling lxml-4.2.6:\n",
            "      Successfully uninstalled lxml-4.2.6\n",
            "  Found existing installation: regex 2019.12.20\n",
            "    Uninstalling regex-2019.12.20:\n",
            "      Successfully uninstalled regex-2019.12.20\n",
            "Successfully installed courlan-0.3.1 cssselect-1.1.0 dateparser-1.0.0 htmldate-0.8.0 justext-2.2.0 lxml-4.6.2 readability-lxml-0.8.1 regex-2020.11.13 tld-0.12.5 trafilatura-0.8.0 urllib3-1.26.3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "urllib3"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_-Fh7PIbiWwo",
        "outputId": "89d42243-64c7-4c4f-b550-dabe200cf655"
      },
      "source": [
        "pip install summa\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting summa\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/45/3b/1c7dc435d05aef474c4137328400f1e11787b9bffab1f87a3f160c1fef54/summa-1.2.0.tar.gz (54kB)\n",
            "\r\u001b[K     |██████                          | 10kB 14.0MB/s eta 0:00:01\r\u001b[K     |████████████                    | 20kB 10.1MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 30kB 8.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 40kB 8.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 51kB 4.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 61kB 3.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=0.19 in /usr/local/lib/python3.7/dist-packages (from summa) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scipy>=0.19->summa) (1.19.5)\n",
            "Building wheels for collected packages: summa\n",
            "  Building wheel for summa (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for summa: filename=summa-1.2.0-cp37-none-any.whl size=54411 sha256=82486b9d5e7245cb263bc93708dd5522bd0237c3677c268f1ccb19cbca002d07\n",
            "  Stored in directory: /root/.cache/pip/wheels/6a/09/68/e2f2861c01d86407c3fa5220826ed7eed2abaa56b001be5970\n",
            "Successfully built summa\n",
            "Installing collected packages: summa\n",
            "Successfully installed summa-1.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JVyYHJ4yibFe",
        "outputId": "bf71853d-3cf1-4c9f-ae37-ce9cb995b206"
      },
      "source": [
        "pip install git+https://github.com/smirnov-am/pytopicrank.git#egg=pytopicrank\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytopicrank\n",
            "  Cloning https://github.com/smirnov-am/pytopicrank.git to /tmp/pip-install-kx5lkz1a/pytopicrank\n",
            "  Running command git clone -q https://github.com/smirnov-am/pytopicrank.git /tmp/pip-install-kx5lkz1a/pytopicrank\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mIpdxGPRjr9L",
        "outputId": "18592340-9a95-4e97-9fc8-b03975f66fbc"
      },
      "source": [
        "pip install git+https://github.com/LIAAD/yake\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/LIAAD/yake\n",
            "  Cloning https://github.com/LIAAD/yake to /tmp/pip-req-build-ovx991hx\n",
            "  Running command git clone -q https://github.com/LIAAD/yake /tmp/pip-req-build-ovx991hx\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from yake==0.4.5) (0.8.9)\n",
            "Requirement already satisfied: click>=6.0 in /usr/local/lib/python3.7/dist-packages (from yake==0.4.5) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from yake==0.4.5) (1.19.5)\n",
            "Collecting segtok\n",
            "  Downloading https://files.pythonhosted.org/packages/41/08/582dab5f4b1d5ca23bc6927b4bb977c8ff7f3a87a3b98844ef833e2f5623/segtok-1.5.10.tar.gz\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from yake==0.4.5) (2.5)\n",
            "Collecting jellyfish\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/30/a6/4d039bc827a102f62ce7a7910713e38fdfd7c7a40aa39c72fb14938a1473/jellyfish-0.8.2-cp37-cp37m-manylinux2014_x86_64.whl (90kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 4.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from segtok->yake==0.4.5) (2020.11.13)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from networkx->yake==0.4.5) (4.4.2)\n",
            "Building wheels for collected packages: yake, segtok\n",
            "  Building wheel for yake (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for yake: filename=yake-0.4.5-py2.py3-none-any.whl size=59918 sha256=6c3c86fa60a95c75be0cc60f3c78a2c656aa909e6ee875a4adc506c99437754c\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-wimo8782/wheels/be/35/27/e4ebd54b78c1806ed8b0271ce247fcd91e2bedde35889fbc9b\n",
            "  Building wheel for segtok (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for segtok: filename=segtok-1.5.10-cp37-none-any.whl size=25019 sha256=a1f90db4d4622120148686deb3b1150a6acd60a0c1be772973547ec4446ca5e4\n",
            "  Stored in directory: /root/.cache/pip/wheels/b4/39/f6/9ca1c5cabde964d728023b5751c3a206a5c8cc40252321fb6b\n",
            "Successfully built yake segtok\n",
            "Installing collected packages: segtok, jellyfish, yake\n",
            "Successfully installed jellyfish-0.8.2 segtok-1.5.10 yake-0.4.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 824
        },
        "id": "fesykwLFkEQE",
        "outputId": "c5f4bf39-5c33-4a90-eae2-dce6ebb5bb49"
      },
      "source": [
        "pip install keyBERT"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting keyBERT\n",
            "  Downloading https://files.pythonhosted.org/packages/5d/f4/65853bfc2ded495af3341a3f8938f17799d15a65be0150cb57774e1fb59f/keybert-0.2.0.tar.gz\n",
            "Requirement already satisfied: sentence-transformers>=0.3.8 in /usr/local/lib/python3.7/dist-packages (from keyBERT) (0.4.1.2)\n",
            "Requirement already satisfied: scikit-learn>=0.22.2 in /usr/local/lib/python3.7/dist-packages (from keyBERT) (0.22.2.post1)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.7/dist-packages (from keyBERT) (1.19.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence-transformers>=0.3.8->keyBERT) (4.41.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers>=0.3.8->keyBERT) (4.3.3)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from sentence-transformers>=0.3.8->keyBERT) (0.1.95)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers>=0.3.8->keyBERT) (1.7.1+cu101)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence-transformers>=0.3.8->keyBERT) (3.2.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers>=0.3.8->keyBERT) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22.2->keyBERT) (1.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers>=0.3.8->keyBERT) (2020.11.13)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers>=0.3.8->keyBERT) (3.7.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers>=0.3.8->keyBERT) (20.9)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers>=0.3.8->keyBERT) (0.0.43)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers>=0.3.8->keyBERT) (3.0.12)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers>=0.3.8->keyBERT) (0.10.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers>=0.3.8->keyBERT) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->sentence-transformers>=0.3.8->keyBERT) (3.7.4.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->sentence-transformers>=0.3.8->keyBERT) (1.15.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers<5.0.0,>=3.1.0->sentence-transformers>=0.3.8->keyBERT) (3.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers<5.0.0,>=3.1.0->sentence-transformers>=0.3.8->keyBERT) (2.4.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=3.1.0->sentence-transformers>=0.3.8->keyBERT) (7.1.2)\n",
            "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/56/aa/4ef5aa67a9a62505db124a5cb5262332d1d4153462eb8fd89c9fa41e5d92/urllib3-1.25.11-py2.py3-none-any.whl (127kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 4.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers>=0.3.8->keyBERT) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers>=0.3.8->keyBERT) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers>=0.3.8->keyBERT) (2.10)\n",
            "Building wheels for collected packages: keyBERT\n",
            "  Building wheel for keyBERT (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keyBERT: filename=keybert-0.2.0-cp37-none-any.whl size=10599 sha256=ffa31e23fe442ba50d9464386e29f036df8f0d658a48530ac356d266884639e8\n",
            "  Stored in directory: /root/.cache/pip/wheels/59/d7/16/04bab6677a4dfa9fd8ab2b350bac915d60f5378b83d6f5a372\n",
            "Successfully built keyBERT\n",
            "\u001b[31mERROR: htmldate 0.8.0 has requirement chardet>=4.0.0; python_version >= \"3.6\", but you'll have chardet 3.0.4 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: keyBERT, urllib3\n",
            "  Found existing installation: urllib3 1.26.3\n",
            "    Uninstalling urllib3-1.26.3:\n",
            "      Successfully uninstalled urllib3-1.26.3\n",
            "Successfully installed keyBERT-0.2.0 urllib3-1.25.11\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "urllib3"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zyOFtszvfI_0"
      },
      "source": [
        "# Restructure"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4WhPGVvhY0RP"
      },
      "source": [
        "def google_search(api_key, search_engine, query, page_number=1):\r\n",
        "  start = (page_number - 1) * 10 + 1\r\n",
        "  url = f\"https://www.googleapis.com/customsearch/v1?key={api_key}&cx={search_engine}&q={query}&start={start}\"\r\n",
        "  data = requests.get(url).json()\r\n",
        "  return data\r\n",
        "  \r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Efk212Enbnx0"
      },
      "source": [
        "def results(data):\r\n",
        "  search_items = data.get(\"items\")\r\n",
        "  for i, search_item in enumerate(search_items, start=1):\r\n",
        "    title = search_item.get(\"title\")\r\n",
        "    snippet = search_item.get(\"snippet\")\r\n",
        "    link = search_item.get(\"link\")\r\n",
        "    print(\"Title:\", title)\r\n",
        "    print(\"Description:\", snippet)\r\n",
        "    print(\"URL:\", link, \"\\n\")\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "omyJOL-bC6CC"
      },
      "source": [
        "def Sort (info):\r\n",
        "  info.sort(key = lambda x: x[1], reverse=True)\r\n",
        "  return info"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_lgx4bldvfo6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "outputId": "132e101e-98d9-4bb2-c519-b8340c8368e0"
      },
      "source": [
        "data =google_search(API_KEY,SEARCH_ENGINE_ID,myquery)\r\n",
        "results(data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-03559b5a7f11>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mgoogle_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAPI_KEY\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mSEARCH_ENGINE_ID\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmyquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'myquery' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mnSMW1Zve5UF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5216ae1-829d-4961-b865-06f843ae4586"
      },
      "source": [
        "##Receive data from the user and run search\r\n",
        "myquery= input(\"Please enter the information to be searched \")\r\n",
        "data =google_search(API_KEY,SEARCH_ENGINE_ID,myquery)\r\n",
        "results(data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Please enter the information to be searched The new architectures directly follow those proposed in our earlier work [13, 14], where it was found that neural network language model can be successfully trained in two steps: first, continuous word vectors are learned using simple model, and then the N-gram NNLM is trained on top of these distributed representations of words\n",
            "Title: Efficient Estimation of Word Representations in Vector Space\n",
            "Description: Sep 7, 2013 ... We propose two novel model architectures for computing continuous vector repre\n",
            "- ... work, we directly extend this architecture, and focus just on the first step \n",
            "where ... found that neural network language model can be successfully trained \n",
            "... word vectors are learned using simple model, and then the N-gram ...\n",
            "URL: https://arxiv.org/pdf/1301.3781 \n",
            "\n",
            "Title: (PDF) Efficient Estimation of Word Representations in Vector Space\n",
            "Description: Oct 21, 2014 ... We propose two novel model architectures for computing continuous vector ... \n",
            "network based language models signiﬁcantly outperform N-gram ... ﬁrst \n",
            "learned using neural network with a single hidden layer. ... The new architectures \n",
            "directly follow those proposed in our earlier work [13, 14], where it was.\n",
            "URL: https://www.researchgate.net/publication/234131319_Efficient_Estimation_of_Word_Representations_in_Vector_Space \n",
            "\n",
            "Title: On word embeddings - Part 1\n",
            "Description: Apr 11, 2016 ... In fact, in many NLP architectures, they have almost completely ... Pre-trained \n",
            "embeddings can then be used in downstream tasks that use small amounts of \n",
            "labeled data. ... Naturally, every feed-forward neural network that takes words \n",
            "from a ... In n-gram based language models, we can calculate a word's ...\n",
            "URL: https://ruder.io/word-embeddings-1/ \n",
            "\n",
            "Title: A Neural Probabilistic Language Model\n",
            "Description: A goal of statistical language modeling is to learn the joint probability function of \n",
            "... Traditional but very successful approaches based on n-grams obtain ... formal \n",
            "presentation will follow in Section 2, using an implementation of these ... layer \n",
            "neural network to predict the next word given the previous ones, in the \n",
            "experiments).\n",
            "URL: https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf \n",
            "\n",
            "Title: Document Embedding Techniques. A review of notable literature on ...\n",
            "Description: Word embedding — the mapping of words into numerical vector spaces ... since \n",
            "the early 2000's — under the name of neural probabilistic language models — it \n",
            "... By simply inputing ye olde bag-of-words into a neural network learning to solve \n",
            "... Then, each document is represented by a vector of length n, in which the i-th ...\n",
            "URL: https://towardsdatascience.com/document-embedding-techniques-fed3e7a6a25d \n",
            "\n",
            "Title: 30 Questions to test a data scientist on Natural Language ...\n",
            "Description: Jul 3, 2017 ... But, what if machines could understand our language and then act accordingly? \n",
            "... We recently launched an NLP skill test on which a total of 817 people \n",
            "registered. ... Natural Language Processing Made Easy – using SpaCy ( in \n",
            "Python) ... A) Training a word 2 vector model on the corpus that learns context ...\n",
            "URL: https://www.analyticsvidhya.com/blog/2017/07/30-questions-test-data-scientist-natural-language-processing-solution-skilltest-nlp/ \n",
            "\n",
            "Title: A hands-on intuitive approach to Deep Learning Methods for Text ...\n",
            "Description: Working with unstructured text data is hard especially when you are trying to \n",
            "build an ... One of the famous papers talking about these semantic word vectors \n",
            "and ... Predictive methods like Neural Network based language models try to \n",
            "predict words ... For one of the corpora, we will reuse our corpus from our \n",
            "previous article, ...\n",
            "URL: https://towardsdatascience.com/understanding-feature-engineering-part-4-deep-learning-methods-for-text-data-96c44370bbfa \n",
            "\n",
            "Title: Deep Learning Architectures for Sequence Processing\n",
            "Description: Dec 30, 2020 ... Chapter 7, depicts the operation of a neural language model using this approach \n",
            "with a window of size 3. Here, we're predicting which word will come next given \n",
            "the in- ... the primary weakness of our earlier Markov N-gram approaches in ... \n",
            "Figure 9.2 Simple recurrent neural network after Elman (Elman, ...\n",
            "URL: https://web.stanford.edu/~jurafsky/slp3/9.pdf \n",
            "\n",
            "Title: Word2vec - Wikipedia\n",
            "Description: Word2vec is a technique for natural language processing. The word2vec \n",
            "algorithm uses a neural network model to learn word associations from a large \n",
            "corpus ...\n",
            "URL: https://en.wikipedia.org/wiki/Word2vec \n",
            "\n",
            "Title: A Gentle Introduction to the Bag-of-Words Model - Machine Learning ...\n",
            "Description: Oct 9, 2017 ... The bag-of-words model is simple to understand and implement and has seen ... \n",
            "Machine learning algorithms cannot work with raw text directly; the text must ... \n",
            "Page 65, Neural Network Methods in Natural Language Processing, 2017. ... of \n",
            "words listed above in our vocabulary, we can step through the first ...\n",
            "URL: https://machinelearningmastery.com/gentle-introduction-bag-words-model/ \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Id30FYhFdZLM"
      },
      "source": [
        "# Using Rake to do keyword extraction to know what to search"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dRn3wJEgAgaX"
      },
      "source": [
        "stop_file ='SmartStoplist.txt'\r\n",
        "myrake = RAKE.Rake(stop_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bGsTFQqIGFtf",
        "outputId": "0490c35d-3788-47c8-f8d0-50df3915f82e"
      },
      "source": [
        "keyword = Sort(myrake.run(myquery))\r\n",
        "print(keyword)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('achieves state-of-theart results', 25.0), ('flexible region-based modulation', 16.0), ('simulated multi-object environments', 16.0), ('rich real-world indoor', 16.0), ('visual generative modeling', 9.0), ('enables longrange interactions', 9.0), ('high-resolution synthesis', 9.0), ('iteratively propagates information', 9.0), ('evolving visual features', 9.0), ('utilizes multiplicative integration', 9.0), ('enjoying fast learning', 9.0), ('quantitative experiments offer', 9.0), ('revealing improved interpretability', 9.0), ('successful stylegan network', 8.5), ('classic transformer architecture', 8.0), ('network employs', 4.5), ('efficient type', 4.0), ('bipartite structure', 4.0), ('maintaining computation', 4.0), ('linearly efficiency', 4.0), ('readily scale', 4.0), ('latent variables', 4.0), ('vice versa', 4.0), ('compositional representations', 4.0), ('careful evaluation', 4.0), ('stronger disentanglement', 4.0), ('outdoor scenes', 3.5), ('image quality', 3.5), ('/dorarad/gansformer', 3.5), ('transformer', 2.0), ('gansformer', 1.5), ('image', 1.5), ('scenes', 1.5), ('introduce', 1.0), ('explore', 1.0), ('task', 1.0), ('set', 1.0), ('support', 1.0), ('refinement', 1.0), ('light', 1.0), ('encourage', 1.0), ('emergence', 1.0), ('objects', 1.0), ('contrast', 1.0), ('generalization', 1.0), ('demonstrate', 1.0), ('model', 1.0), ('strength', 1.0), ('robustness', 1.0), ('range', 1.0), ('datasets', 1.0), ('showing', 1.0), ('terms', 1.0), ('diversity', 1.0), ('dataefficiency', 1.0), ('qualitative', 1.0), ('insight', 1.0), ('workings', 1.0), ('illustrating', 1.0), ('benefits', 1.0), ('efficacy', 1.0), ('approach', 1.0), ('implementation', 1.0), ('https', 1.0), ('//github', 1.0)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "XI3nn3HfNPmg",
        "outputId": "d26f809a-e70e-41e0-925b-030e1257acd8"
      },
      "source": [
        "r = Rake(ranking_metric=Metric.DEGREE_TO_FREQUENCY_RATIO)\r\n",
        "r = Rake(min_length=2, max_length=4)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-6864d57d9076>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mranking_metric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMetric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDEGREE_TO_FREQUENCY_RATIO\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Rake' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "id": "c4_pXotxSfuR",
        "outputId": "401cf93c-45dc-4444-9d10-099956546ebb"
      },
      "source": [
        "myquery= input(\"Please enter the information to be searched \")\r\n",
        "data =google_search(API_KEY,SEARCH_ENGINE_ID,myquery)\r\n",
        "results(data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Please enter the information to be searched W e   d e m o n s t r a t e   t h e   m o d e l ’ s   s t r e n g t h   a n d   r o b u s t n e s s   t h r o u g h   a   c a r e f u l   e v a l u a t i o n   o v e r   a   r a n g e   o f   d a t a s e t s ,   f r o m   s i m u l a t e d   m u l t i - o b j e c t   e n v i r o n m e n t s   t o   r i c h   r e a l - w o r l d   i n d o o r   a n d   o u t d o o r   s c e n e s ,   s h o w i n g   i t   a c h i e v e s   s t a t e - o f - t h e a r t   r e s u l t s   i n   t e r m s   o f   i m a g e   q u a l i t y   a n d   d i v e r s i t y ,   w h i l e   e n j o y i n g   f a s t   l e a r n i n g   a n    better   data efficiency\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-48bc6e66e4f1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmyquery\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Please enter the information to be searched \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mgoogle_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAPI_KEY\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mSEARCH_ENGINE_ID\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmyquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mresults\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-12-e49897839d01>\u001b[0m in \u001b[0;36mresults\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0msearch_items\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"items\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msearch_item\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msearch_items\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mtitle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msearch_item\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"title\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0msnippet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msearch_item\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"snippet\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HyL5RMapdtFt"
      },
      "source": [
        "#Using NLTK- Rake"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HwZ5G33YS-nu"
      },
      "source": [
        "from rake_nltk import Rake,Metric"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j7UttdyfVW-V",
        "outputId": "2be94e5b-65c1-4299-b73b-9e27948eda3c"
      },
      "source": [
        "print(myquery)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "We introduce the GANsformer, a novel and efficient type of transformer, and explore it for the task of visual generative modeling. The network employs a bipartite structure that enables longrange interactions across the image, while maintaining computation of linearly efficiency, that can readily scale to high-resolution synthesis. It iteratively propagates information from a set of latent variables to the evolving visual features and vice versa, to support the refinement of each in light of the other and encourage the emergence of compositional representations of objects and scenes. In contrast to the classic transformer architecture, it utilizes multiplicative integration that allows flexible region-based modulation, and can thus be seen as a generalization of the successful StyleGAN network. We demonstrate the model’s strength and robustness through a careful evaluation over a range of datasets, from simulated multi-object environments to rich real-world indoor and out\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "okEwNXLhTtZn",
        "outputId": "4f9809a1-2cc3-488b-fd09-9dbd2dcdbff0"
      },
      "source": [
        "myrake2 = Rake()\r\n",
        "myrake2.extract_keywords_from_text(myquery)\r\n",
        "myrake2.get_ranked_phrases()\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['iteratively propagates information',\n",
              " 'evolving visual features',\n",
              " 'enjoying fast learning',\n",
              " 'world indoor',\n",
              " 'vice versa',\n",
              " 'theart results',\n",
              " 'simulated multi',\n",
              " 'rich real',\n",
              " 'object environments',\n",
              " 'model ’',\n",
              " 'latent variables',\n",
              " 'image quality',\n",
              " 'compositional representations',\n",
              " 'careful evaluation',\n",
              " 'better dataefficiency',\n",
              " 'achieves state',\n",
              " 'outdoor scenes',\n",
              " 'scenes',\n",
              " 'terms',\n",
              " 'support',\n",
              " 'strength',\n",
              " 'showing',\n",
              " 'set',\n",
              " 'robustness',\n",
              " 'refinement',\n",
              " 'range',\n",
              " 'objects',\n",
              " 'light',\n",
              " 'encourage',\n",
              " 'emergence',\n",
              " 'diversity',\n",
              " 'demonstrate',\n",
              " 'datasets']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JfuJPLtROgqO",
        "outputId": "f716229b-374d-4a4e-e13c-ec479b03f768"
      },
      "source": [
        "r = Rake(ranking_metric=Metric.WORD_DEGREE)\r\n",
        "#r = Rake(min_length=4, max_length=5)\r\n",
        "r.extract_keywords_from_text(myquery)\r\n",
        "r.get_ranked_phrases()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['iteratively propagates information',\n",
              " 'evolving visual features',\n",
              " 'enjoying fast learning',\n",
              " 'outdoor scenes',\n",
              " 'world indoor',\n",
              " 'vice versa',\n",
              " 'theart results',\n",
              " 'simulated multi',\n",
              " 'rich real',\n",
              " 'object environments',\n",
              " 'model ’',\n",
              " 'latent variables',\n",
              " 'image quality',\n",
              " 'compositional representations',\n",
              " 'careful evaluation',\n",
              " 'better dataefficiency',\n",
              " 'achieves state',\n",
              " 'scenes',\n",
              " 'terms',\n",
              " 'support',\n",
              " 'strength',\n",
              " 'showing',\n",
              " 'set',\n",
              " 'robustness',\n",
              " 'refinement',\n",
              " 'range',\n",
              " 'objects',\n",
              " 'light',\n",
              " 'encourage',\n",
              " 'emergence',\n",
              " 'diversity',\n",
              " 'demonstrate',\n",
              " 'datasets']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-h4nj6bvNxlD",
        "outputId": "ef8b004c-6de6-4c6a-fffa-856e3ef16090"
      },
      "source": [
        "r = Rake(ranking_metric=Metric.DEGREE_TO_FREQUENCY_RATIO)\r\n",
        "#r = Rake(min_length=4, max_length=5)\r\n",
        "r.extract_keywords_from_text(myquery)\r\n",
        "r.get_ranked_phrases()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['iteratively propagates information',\n",
              " 'evolving visual features',\n",
              " 'enjoying fast learning',\n",
              " 'world indoor',\n",
              " 'vice versa',\n",
              " 'theart results',\n",
              " 'simulated multi',\n",
              " 'rich real',\n",
              " 'object environments',\n",
              " 'model ’',\n",
              " 'latent variables',\n",
              " 'image quality',\n",
              " 'compositional representations',\n",
              " 'careful evaluation',\n",
              " 'better dataefficiency',\n",
              " 'achieves state',\n",
              " 'outdoor scenes',\n",
              " 'scenes',\n",
              " 'terms',\n",
              " 'support',\n",
              " 'strength',\n",
              " 'showing',\n",
              " 'set',\n",
              " 'robustness',\n",
              " 'refinement',\n",
              " 'range',\n",
              " 'objects',\n",
              " 'light',\n",
              " 'encourage',\n",
              " 'emergence',\n",
              " 'diversity',\n",
              " 'demonstrate',\n",
              " 'datasets']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QfO1tdiOh_ya",
        "outputId": "20da26f6-321f-484a-f804-0c5d7bb55111"
      },
      "source": [
        "from summa import keywords\r\n",
        "print(keywords.keywords(myquery))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "efficiency\n",
            "efficient type\n",
            "linearly\n",
            "transformer\n",
            "propagates\n",
            "region\n",
            "flexible\n",
            "careful evaluation\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eRV0W97RnEDt",
        "outputId": "8a0a07db-fe13-4ede-e8b7-e4b9c57ba656"
      },
      "source": [
        "print(myquery)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "optimization object detection\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oIMd_6PUpsOy",
        "outputId": "1de008cc-4984-44c2-e980-561f32751b15"
      },
      "source": [
        "from yake import KeywordExtractor\r\n",
        "kw_extractor = KeywordExtractor(lan=\"en\", n=3, top=5)\r\n",
        "keywords = kw_extractor.extract_keywords(myquery)\r\n",
        "keywords = [x for x, y in keywords]\r\n",
        "print(keywords)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['simulated multi-object environments', 'rich real-world indoor', 'enjoying fast learning', 'showing it achieves', 'range of datasets']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0-_T9SkeDos"
      },
      "source": [
        "# Using CountVectorizer and Distilbert to do\r\n",
        "\r\n",
        "*   List item\r\n",
        "*   List item\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "klgOdpiqWAo7"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\r\n",
        "\r\n",
        "n_gram_range = (3, 3)\r\n",
        "stop_words = \"english\"\r\n",
        "\r\n",
        "count = CountVectorizer(ngram_range=n_gram_range, stop_words=stop_words).fit([myquery])\r\n",
        "candidates = count.get_feature_names()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zLJ9CnN2P5Y9",
        "outputId": "73f7b46c-30db-4afd-f31e-b3ad69d8dc71"
      },
      "source": [
        "print(candidates)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['achieves state theart', 'allows flexible region', 'approach implementation model', 'architecture utilizes multiplicative', 'available https github', 'based modulation seen', 'benefits efficacy approach', 'better dataefficiency qualitative', 'bipartite structure enables', 'careful evaluation range', 'classic transformer architecture', 'com dorarad gansformer', 'compositional representations objects', 'computation linearly efficiency', 'contrast classic transformer', 'dataefficiency qualitative quantitative', 'datasets simulated multi', 'demonstrate model strength', 'disentanglement illustrating benefits', 'diversity enjoying fast', 'efficacy approach implementation', 'efficiency readily scale', 'efficient type transformer', 'emergence compositional representations', 'employs bipartite structure', 'enables longrange interactions', 'encourage emergence compositional', 'enjoying fast learning', 'environments rich real', 'evaluation range datasets', 'evolving visual features', 'experiments offer insight', 'explore task visual', 'fast learning better', 'features vice versa', 'flexible region based', 'gansformer novel efficient', 'generalization successful stylegan', 'generative modeling network', 'github com dorarad', 'high resolution synthesis', 'https github com', 'illustrating benefits efficacy', 'image maintaining computation', 'image quality diversity', 'implementation model available', 'improved interpretability stronger', 'indoor outdoor scenes', 'information set latent', 'inner workings revealing', 'insight model inner', 'integration allows flexible', 'interactions image maintaining', 'interpretability stronger disentanglement', 'introduce gansformer novel', 'iteratively propagates information', 'latent variables evolving', 'learning better dataefficiency', 'light encourage emergence', 'linearly efficiency readily', 'longrange interactions image', 'maintaining computation linearly', 'model available https', 'model inner workings', 'model strength robustness', 'modeling network employs', 'modulation seen generalization', 'multi object environments', 'multiplicative integration allows', 'network demonstrate model', 'network employs bipartite', 'novel efficient type', 'object environments rich', 'objects scenes contrast', 'offer insight model', 'outdoor scenes showing', 'propagates information set', 'qualitative quantitative experiments', 'quality diversity enjoying', 'quantitative experiments offer', 'range datasets simulated', 'readily scale high', 'real world indoor', 'refinement light encourage', 'region based modulation', 'representations objects scenes', 'resolution synthesis iteratively', 'results terms image', 'revealing improved interpretability', 'rich real world', 'robustness careful evaluation', 'scale high resolution', 'scenes contrast classic', 'scenes showing achieves', 'seen generalization successful', 'set latent variables', 'showing achieves state', 'simulated multi object', 'state theart results', 'strength robustness careful', 'stronger disentanglement illustrating', 'structure enables longrange', 'stylegan network demonstrate', 'successful stylegan network', 'support refinement light', 'synthesis iteratively propagates', 'task visual generative', 'terms image quality', 'theart results terms', 'transformer architecture utilizes', 'transformer explore task', 'type transformer explore', 'utilizes multiplicative integration', 'variables evolving visual', 'versa support refinement', 'vice versa support', 'visual features vice', 'visual generative modeling', 'workings revealing improved', 'world indoor outdoor']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CVA-RQtcQ4qJ"
      },
      "source": [
        "from sentence_transformers import SentenceTransformer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0IkgyxjBXnC2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ec3c559-5ad6-4036-beca-95b6c4a59448"
      },
      "source": [
        "#from sentence_transformers import SentenceTransformer\r\n",
        "\r\n",
        "model = SentenceTransformer('distilbert-base-nli-mean-tokens')\r\n",
        "doc_embedding = model.encode([myquery])\r\n",
        "candidate_embeddings = model.encode(candidates)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 245M/245M [00:13<00:00, 17.5MB/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_GdE0lyncoo0"
      },
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\r\n",
        "\r\n",
        "top_n = 5\r\n",
        "distances = cosine_similarity(doc_embedding, candidate_embeddings)\r\n",
        "keywords = [candidates[index] for index in distances.argsort()[0][-top_n:]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iqf3zG8lczuc",
        "outputId": "414dcd0e-f234-4b63-c51f-f11b647b6360"
      },
      "source": [
        "print(keywords)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['multiplicative integration allows', 'encourage emergence compositional', 'resolution synthesis iteratively', 'synthesis iteratively propagates', 'efficient type transformer']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SVyPaMFLfGBl",
        "outputId": "b57e5a46-d70b-4e9b-afb3-71c2a762c723"
      },
      "source": [
        "#Receive data from the user and run search\r\n",
        "myquery= input(\"Please enter the information to be searched \")\r\n",
        "data =google_search(API_KEY,SEARCH_ENGINE_ID,myquery)\r\n",
        "results(data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Please enter the information to be searched We demonstrate the model’s strength and robustness through a careful evaluation over a range of datasets, from simulated multi-object environments to rich real-world indoor and outdoor scenes, showing it achieves state-of-theart results in terms of image quality and diversity, while enjoying fast learning and better dataefficiency. It iteratively propagates information from a set of latent variables to the evolving visual features and vice versa, to support the refinement of each in light of the other and encourage the emergence of compositional representations of objects and scenes\n",
            "Title: [2103.01209] Generative Adversarial Transformers\n",
            "Description: 2 days ago ... We demonstrate the model's strength and robustness through a careful \n",
            "evaluation over a range of datasets, from simulated multi-object environments to \n",
            "rich real-world indoor and outdoor scenes, showing it achieves state-of-the-art \n",
            "results in terms of image quality and diversity, while enjoying fast learning ...\n",
            "URL: https://arxiv.org/abs/2103.01209 \n",
            "\n",
            "Title: Arxiv Sanity Preserver\n",
            "Description: Top papers mentioned on Twitter over last day: ... We demonstrate the model's \n",
            "strength and robustness through a careful evaluation over a range of datasets, \n",
            "from simulated multi-object environments to rich real-world indoor and outdoor \n",
            "scenes, showing it achieves state-of-the-art results in terms of image quality and\n",
            " ...\n",
            "URL: http://www.arxiv-sanity.com/toptwtr \n",
            "\n",
            "Title: Publications - Vladlen Koltun\n",
            "Description: The goal state can be specified by object poses, by images, by a description in ... \n",
            "multi-resolution features allow us to train a single model on a diverse set of tasks \n",
            "and ... We evaluate the presented approach on challenging real-world datasets, \n",
            "... We show that dynamic scenes can be reconstructed from a burst of frames at a\n",
            " ...\n",
            "URL: http://vladlen.info/publications/ \n",
            "\n",
            "Title: Arxiv Sanity Preserver\n",
            "Description: We show that our method leads to a 90% accuracy rate of detecting ... strength \n",
            "and robustness through a careful evaluation over a range of datasets, from \n",
            "simulated multi-object environments to rich real-world indoor and outdoor scenes\n",
            ", showing it achieves state-of-the-art results in terms of image quality and \n",
            "diversity, while ...\n",
            "URL: http://www.arxiv-sanity.com/toptwtr?timefilter=week \n",
            "\n",
            "Title: ACM Transactions on Graphics (TOG) - Proceedings of ACM ...\n",
            "Description: The quality of the resulting motions heavily depends on this component, ... \n",
            "improve the quality of simulations by expanding the applicable range of motion \n",
            "predictions. ... During subsequent posing of the 3D character models, they use \n",
            "these ... We show that recent works on scene editing and gradient-domain \n",
            "rendering can ...\n",
            "URL: https://www.siggraph.org/sites/default/files/SIGGRPAH-Asia-2016-Opentoc_1.html \n",
            "\n",
            "Title: Critical analysis of Big Data challenges and analytical methods ...\n",
            "Description: Big Data (BD), with their potential to ascertain valued insights for enhanced ... \n",
            "nature of the BD and BDA, this paper presents a state-of-the-art review that \n",
            "presents ... data, how to select the right model for analysis and how to provide the \n",
            "results. ... 1 shows the classification of BD challenges – as adapted from Akerkar \n",
            "(2014) ...\n",
            "URL: https://www.sciencedirect.com/science/article/pii/S014829631630488X \n",
            "\n",
            "Title: 15th International Conference on Computer Vision ... - VISAPP 2020\n",
            "Description: We also present measurements on moving objects in the scene and ... The \n",
            "results of our experiments on real images and synthetic images show that the ... \n",
            "Although THz imaging is a useful tool in many applications, there are several \n",
            "effects of a ... models for 3D shapes is a fundamental task in computer vision with \n",
            "diverse ...\n",
            "URL: http://www.visapp.visigrapp.org/Abstracts/2020/VISAPP_2020_Abstracts.htm \n",
            "\n",
            "Title: ICML 2020\n",
            "Description: Jul 16, 2020 ... The model first predicts the set of graph edits transforming the target into \n",
            "incomplete ... For 5- and 6-node motifs, we show that SPMiner can identify almost \n",
            "all of the ... and inductive learning tasks using a variety of real-world datasets. ... \n",
            "On real datasets, we achieve state-of-the-art results at a fraction of the ...\n",
            "URL: https://icml.cc/Conferences/2020/ScheduleMultitrack?event=5715 \n",
            "\n",
            "Title: ETRA '19- Proceedings of the 11th ACM Symposium on Eye ...\n",
            "Description: Gaze behaviour on interacted objects during hand interaction in virtual reality for \n",
            "eye ... The results show the proposed method is fully functional, quick, and \n",
            "enables ... By inspecting real-life recordings of egocentric eye tracker cameras, \n",
            "we ... Through in-depth evaluations on a recent mobile eye tracking dataset (N=\n",
            "17, ...\n",
            "URL: http://st.sigchi.org/publications/toc/etra-2019.html \n",
            "\n",
            "Title: Robust Visual Localization Across Seasons | Request PDF\n",
            "Description: Request PDF | Robust Visual Localization Across Seasons | Localization is an \n",
            "integral part ... We perform extensive evaluations on a variety of datasets and \n",
            "show that our ... Outdoor + Indoor World ∼4.2M GPS Mapillary SLS [252] 2020 \n",
            "Urban World ... algorithm that achieves state-of-the-art performance on several \n",
            "datasets.\n",
            "URL: https://www.researchgate.net/publication/322709839_Robust_Visual_Localization_Across_Seasons \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nYls3qPhtCUt"
      },
      "source": [
        "\r\n",
        "import numpy as np\r\n",
        "import itertools\r\n",
        "\r\n",
        "def max_sum_sim(doc_embedding, word_embeddings, words, top_n, nr_candidates):\r\n",
        "    # Calculate distances and extract keywords\r\n",
        "    distances = cosine_similarity(doc_embedding, candidate_embeddings)\r\n",
        "    distances_candidates = cosine_similarity(candidate_embeddings, \r\n",
        "                                            candidate_embeddings)\r\n",
        "\r\n",
        "    # Get top_n words as candidates based on cosine similarity\r\n",
        "    words_idx = list(distances.argsort()[0][-nr_candidates:])\r\n",
        "    words_vals = [candidates[index] for index in words_idx]\r\n",
        "    distances_candidates = distances_candidates[np.ix_(words_idx, words_idx)]\r\n",
        "\r\n",
        "    # Calculate the combination of words that are the least similar to each other\r\n",
        "    min_sim = np.inf\r\n",
        "    candidate = None\r\n",
        "    for combination in itertools.combinations(range(len(words_idx)), top_n):\r\n",
        "        sim = sum([distances_candidates[i][j] for i in combination for j in combination if i != j])\r\n",
        "        if sim < min_sim:\r\n",
        "            candidate = combination\r\n",
        "            min_sim = sim\r\n",
        "\r\n",
        "    return [words_vals[idx] for idx in candidate]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l5-_0xhFteFZ",
        "outputId": "8392bb19-c4eb-4c29-ce40-e48c80719434"
      },
      "source": [
        "\r\n",
        "max_sum_sim(doc_embedding, candidate_embeddings, candidates, top_n=5, nr_candidates=30)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['tracking detection pipeline',\n",
              " 'work separate optimization',\n",
              " 'essential learning discriminative',\n",
              " 'work propose new',\n",
              " 'modules trained separately']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9T5vrIqyvaRt",
        "outputId": "2c4f4671-fdf9-4ddb-9629-641a3760264c"
      },
      "source": [
        "print(myquery)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Abstract— Object detection and data association are critical components in multi-object tracking (MOT) systems. Despite the fact that the two components are dependent on each other, prior work often designs detection and data association modules separately which are trained with different objectives. As a result, we cannot back-propagate the gradients and optimize the entire MOT system, which leads to sub-optimal performance. To address this issue, recent work simultaneously optimizes detection and data association modules under a joint MOT framework, which has shown improved performance in both modules. In this work, we propose a new instance of joint MOT approach based on Graph Neural Networks (GNNs). The key idea is that GNNs can model relations between variablesized objects in both the spatial and temporal domains, which is essential for learning discriminative features for detection and data association. Through extensive experiments on the MOT15/16/17/20 datasets, we demonstrate the effectiveness of our GNN-based joint MOT approach and show the state-ofthe-art performance for both detection and MOT tasks. I. INTRODUCTION Object detection [1]–[10] and data association [11]–[18] are two components of multi-object tracking (MOT), which is essential to perception in robotic systems such as autonomous driving [19]–[23] and assistive robots [24]–[26]. Prior work [12], [16] often approaches MOT in an online fashion using a tracking-by-detection pipeline, where a detector outputs detections followed by a data association module matching the detections with past tracklets to form new tracklets up to the current frame. Oftentimes, the detector and data association modules are trained separately in prior work. However, with this separate optimization procedure, we cannot back-propagate errors through the entire MOT system. In other words, each module is optimized only towards its own local optimum, but not towards the objective of MOT. As a result, this separate optimization procedure used in prior work often yields sub-optimal performance. To improve performance, we investigate 1) joint optimization of object detection and data association, which we refer to as the joint MOT framework; 2) within the joint MOT framework, how to learn more discriminative features. First, to address the joint MOT problem, prior work [27]–[36] has explored different directions. [27]–[30] proposed to unify object detector with a model-free single-object tracker, where the tracker directly regresses the location of each object detected in the previous frame to the current frame. As each object is tracked independently, the data association problem is naturally resolved. [31]–[34] proposed to extend an object detector by adding a re-identification (Re-ID) [37], [38] or ID verification branch which extracts features of objects\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VTRueNFdvGZw",
        "outputId": "f5482adf-9bce-4a18-8dee-06eee4d94de9"
      },
      "source": [
        "from keybert import KeyBERT\r\n",
        "kw_extractor = KeyBERT('distilbert-base-nli-mean-tokens')\r\n",
        "keywords = kw_extractor.extract_keywords(myquery, stop_words='english')\r\n",
        "print(keywords)\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('experiments', 0.3752), ('optimizes', 0.3703), ('optimization', 0.3682), ('trained', 0.3548), ('optimize', 0.3521)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ld-elu5qnULh"
      },
      "source": [
        "PRELIM WITH BERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yUE-QS5dnTBZ",
        "outputId": "b416acc1-d6ee-4560-c313-7fa604e9950a"
      },
      "source": [
        "pip install bert-for-tf2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting bert-for-tf2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a5/a1/acb891630749c56901e770a34d6bac8a509a367dd74a05daf7306952e910/bert-for-tf2-0.14.9.tar.gz (41kB)\n",
            "\r\u001b[K     |████████                        | 10kB 17.1MB/s eta 0:00:01\r\u001b[K     |████████████████                | 20kB 11.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 30kB 8.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 40kB 7.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 51kB 4.3MB/s \n",
            "\u001b[?25hCollecting py-params>=0.9.6\n",
            "  Downloading https://files.pythonhosted.org/packages/aa/e0/4f663d8abf83c8084b75b995bd2ab3a9512ebc5b97206fde38cef906ab07/py-params-0.10.2.tar.gz\n",
            "Collecting params-flow>=0.8.0\n",
            "  Downloading https://files.pythonhosted.org/packages/a9/95/ff49f5ebd501f142a6f0aaf42bcfd1c192dc54909d1d9eb84ab031d46056/params-flow-0.8.2.tar.gz\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (1.19.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (4.41.1)\n",
            "Building wheels for collected packages: bert-for-tf2, py-params, params-flow\n",
            "  Building wheel for bert-for-tf2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bert-for-tf2: filename=bert_for_tf2-0.14.9-cp37-none-any.whl size=30535 sha256=e8a85a0bc7324394fb93578c3f28ea6298c9588c467985ac8fa2962df7b18343\n",
            "  Stored in directory: /root/.cache/pip/wheels/a1/04/ee/347bd9f5b821b637c76411d280271a857aece00358896a230f\n",
            "  Building wheel for py-params (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for py-params: filename=py_params-0.10.2-cp37-none-any.whl size=7912 sha256=677e6dbf3bf9745ec3986be6a4643b632c8086b842fe99fb6f950785a1f6e9fa\n",
            "  Stored in directory: /root/.cache/pip/wheels/d0/4a/70/ff12450229ff1955abf01f365051d4faae1c20aef53ab4cf09\n",
            "  Building wheel for params-flow (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for params-flow: filename=params_flow-0.8.2-cp37-none-any.whl size=19472 sha256=97c18e37f1138bf2343cb58d6e23734cfa04826f45a9c76ff3638904d9c9b433\n",
            "  Stored in directory: /root/.cache/pip/wheels/08/c8/7f/81c86b9ff2b86e2c477e3914175be03e679e596067dc630c06\n",
            "Successfully built bert-for-tf2 py-params params-flow\n",
            "Installing collected packages: py-params, params-flow, bert-for-tf2\n",
            "Successfully installed bert-for-tf2-0.14.9 params-flow-0.8.2 py-params-0.10.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z1PZS9RsnTQ1"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "import tensorflow_hub as hub\r\n",
        "from tensorflow.keras import layers\r\n",
        "import bert\r\n",
        "\r\n",
        "BertTokenizer = bert.bert_tokenization.FullTokenizer\r\n",
        "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",\r\n",
        "                            trainable=False)\r\n",
        "\r\n",
        "\r\n",
        "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\r\n",
        "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\r\n",
        "tokenizer = BertTokenizer(vocab_file, do_lower_case)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QcrmB48CnTTh",
        "outputId": "a017b2c9-d18d-4a5f-c79e-0ca86d45b956"
      },
      "source": [
        "text = \"This is a little confusing \"\r\n",
        "# tokenize\r\n",
        "tokens_list = tokenizer.tokenize(text)\r\n",
        "print('Text after tokenization')\r\n",
        "print(tokens_list)\r\n",
        "\r\n",
        "# initilize dimension\r\n",
        "max_len =25\r\n",
        "text = tokens_list[:max_len-2]\r\n",
        "input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\r\n",
        "print(\"After adding  flasges -[CLS] and [SEP]: \")\r\n",
        "print(input_sequence)\r\n",
        "\r\n",
        "\r\n",
        "tokens = tokenizer.convert_tokens_to_ids(input_sequence )\r\n",
        "print(\"tokens to id \")\r\n",
        "print(tokens)\r\n",
        "\r\n",
        "pad_len = max_len -len(input_sequence)\r\n",
        "tokens += [0] * pad_len\r\n",
        "print(\"tokens: \")\r\n",
        "print(tokens)\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Text after tokenization\n",
            "['this', 'is', 'a', 'little', 'confusing']\n",
            "After adding  flasges -[CLS] and [SEP]: \n",
            "['[CLS]', 'this', 'is', 'a', 'little', 'confusing', '[SEP]']\n",
            "tokens to id \n",
            "[101, 2023, 2003, 1037, 2210, 16801, 102]\n",
            "tokens: \n",
            "[101, 2023, 2003, 1037, 2210, 16801, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GB65-Ym5Y8RJ"
      },
      "source": [
        "Passage Level\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RCIs2spcQ6YE",
        "outputId": "2bf2b9ac-04c1-43bb-a0a3-3f6314e2fbe4"
      },
      "source": [
        "from nltk.tokenize import sent_tokenize\r\n",
        "#text = \"God is Great! I won a lottery.\"\r\n",
        "sentences=sent_tokenize(myquery)\r\n",
        "print(sentences)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['We introduce the GANsformer, a novel and efficient type of transformer, and explore it for the task of visual generative modeling.', 'The network employs a bipartite structure that enables longrange interactions across the image, while maintaining computation of linearly efficiency, that can readily scale to high-resolution synthesis.', 'It iteratively propagates information from a set of latent variables to the evolving visual features and vice versa, to support the refinement of each in light of the other and encourage the emergence of compositional representations of objects and scenes.', 'In contrast to the classic transformer architecture, it utilizes multiplicative integration that allows flexible region-based modulation, and can thus be seen as a generalization of the successful StyleGAN network.', 'We demonstrate the model’s strength and robustness through a careful evaluation over a range of datasets, from simulated multi-object environments to rich real-world indoor and outdoor scenes, showing it achieves state-of-theart results in terms of image quality and diversity, while enjoying fast learning and better dataefficiency.', 'Further qualitative and quantitative experiments offer us an insight into the model’s inner workings, revealing improved interpretability and stronger disentanglement, and illustrating the benefits and efficacy of our approach.', 'An implementation of the model is available at https: //github.com/dorarad/gansformer.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMQrKH7hnTWy"
      },
      "source": [
        "def build_similarity_matrix(sentences, stop_words):\r\n",
        "  # Create an empty similarity matrix\r\n",
        "  similarity_matrix = np.zeros((len(sentences), len(sentences)))\r\n",
        "  for idx1 in range(len(sentences)):0.\r\n",
        "  \r\n",
        "       continue \r\n",
        "      similarity_matrix[idx1][idx2] = sentence_similarity(sentences[idx1], sentences[idx2], stop_words)\r\n",
        "  return similarity_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B__fcP8lU1id"
      },
      "source": [
        "import numpy as np\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bLLN5AHdol2M",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "outputId": "e494269c-1e44-4baa-9b48-fa0c488543f6"
      },
      "source": [
        "summarize_text = []\r\n",
        "sentence_similarity_martix = build_similarity_matrix(sentences, stop_words)\r\n",
        "sentence_similarity_graph = nx.from_numpy_array(sentence_similarity_martix)\r\n",
        "scores = nx.pagerank(sentence_similarity_graph)\r\n",
        "ranked_sentence = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)    \r\n",
        "print(\"Indexes of top ranked_sentence order are \", ranked_sentence)\r\n",
        "for i in range(top_n):\r\n",
        "  summarize_text.append(\" \".join(ranked_sentence[i][1]))\r\n",
        "  # Step 5 - Offcourse, output the summarize texr\r\n",
        "  print(\"Summarize Text: \\n\", \". \".join(summarize_text))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-80d7fc967e0a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msummarize_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msentence_similarity_martix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_similarity_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0msentence_similarity_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence_similarity_martix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpagerank\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence_similarity_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mranked_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-33-81bf8876562d>\u001b[0m in \u001b[0;36mbuild_similarity_matrix\u001b[0;34m(sentences, stop_words)\u001b[0m\n\u001b[1;32m      6\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0midx1\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0midx2\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#ignore if both are same sentences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m        \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m       \u001b[0msimilarity_matrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentence_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0msimilarity_matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'sentence_similarity' is not defined"
          ]
        }
      ]
    }
  ]
}